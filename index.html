<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- remove after done debugging!!!! -->
<!-- <meta http-equiv="refresh" content="1" > -->
<!-- remove after done debugging!!!! -->

<head>
  <!-- <meta name=viewport content="width=800"> -->
  <meta name="viewport" content="width=device-width">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 30px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700;
      margin-bottom: 10cm;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
 
     .zero {
      width: 160px;
      height: 80px;
      position: relative;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    mid {
      font-size: 40px;
      position:relative;
      top:2px;
    }


    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" href="https://www.ri.cmu.edu/wp-content/uploads/2017/04/ri-favicon.ico">

  <title>Sudharshan Suresh</title>
  
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="75%" valign="middle">
              <p align="center">
                <name>Sudharshan Suresh <mid><font color=#FF0000>|</font></mid> <font color=#C0C0C0>Suddhu</font></name>
              <p align="center">
                <font size="3">suddhu <font color=#FF0000>[at]</font> cmu <font color=#FF0000>[dot]</font> edu </font>
              </p>
              <p><p style = "text-align:justify">I'm a PhD student in the <A href="http://www.ri.cmu.edu/" target="_blank">Robotics Institute</A> at <A href="http://www.cmu.edu" target="_blank"> Carnegie Mellon University</A>,
                 where I work with <A href="https://www.cs.cmu.edu/~kaess/" target="_blank">Michael Kaess</a> in the <A href="http://rpl.ri.cmu.edu/" target="_blank">Robot Perception Lab</a>. I'm broadly interested in statistical methods for robot perception and state-estimation.
              </p>
              <p><p style = "text-align:justify"> I completed my Masters in Robotics at CMU working with <A href="https://www.cs.cmu.edu/~kaess/" target="_blank">Michael Kaess</a> on underwater localization and active SLAM for AUVs.
                Prior to that, I worked with <A href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">Red Whittaker</a> on visual state-estimation for lunar rovers, and spent time at <A href="https://www.iisc.ac.in/">IISc Bangalore</a> working on visual understanding.
                In my undergrad, I majored in Controls and Instrumentation at <A href="https://www.nitt.edu/" target="_blank">NIT Trichy</a>. 
              </p>

              <p align=center>
                <strong>
                <a href="misc/CV_Suddhu.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=xYC738YAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.ri.cmu.edu/ri-people/sudharshan-suresh/" target="_blank"> RI webpage </a> &nbsp/&nbsp
                <a href="https://github.com/suddhu/" target="_blank"> Github </a>
                </strong>
              </p>
            </td>
            <td width="50%">
              <img src="misc/suddhu-headshot.jpg" style="width: 200; height: auto">
            </td>
          </tr>
        </table>

        <div style="height:20px;font-size:1px;">&nbsp;</div>

        <!-- Research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td width="100%" valign="middle">
              <h1>Research</h1>
            </td>
          </tr>
        </table>    

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <!-- current -->
            <td width="5%">          
                <img src='data/media/contact_slam/sim.gif' width="120">
            </td>
            <td width="5%">    
                <img src='data/media/contact_slam/rect1.gif' width="120">      
            </td>
            <td valign="center" width="50%", bgcolor="#FFFFE0">
              <papertitle><font color=#FF0000><strong>(ongoing)</strong></font> Incremental shape and pose estimation from planar pushing using contact implicit surfaces
              </papertitle>
              <div style="height:5px;font-size:1px;">&nbsp;</div>
              <u>S. Suresh</u>,
              <a href="http://robots.engin.umich.edu/~joshuagm/" target="_blank">J. Mangelson</a>, and
              <a href="https://www.cs.cmu.edu/~kaess/" target="_blank">M. Kaess</a>
              <div style="height:5px;font-size:1px;">&nbsp;</div>
              <em>ICRA 2020 workshop - ViTac 2020: Closing the Perception-Action Loop with Vision and Tactile Sensing</em>
              <br>
              <br>
              (<a href="http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2020/05/ICRA2020ViTac_paper_6.pdf" target="_blank"><b>pdf</b></a> /
              <a href="" target="_blank"><b>video</b></a>)

              <!-- <p> Robots need accurate, online estimates of shape and pose while manipulating unknown objects. Interestingly&mdash;even when blindfolded&mdash;humans can
                infer object properties from local tactile information. We investigate combining Gaussian process implicit surfaces with incremental solvers to 
                localize and infer the shape of objects</p> -->
            </td>
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

        <!-- active slam -->
        <tr>
          <td width="25%" align="center">
            <img src='data/media/active_sal/active.gif' width="200">
          </td>
          <td valign="center" width="75%">
            <papertitle>Active SLAM using 3D submap saliency for underwater volumetric exploration
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>S. Suresh</u>,
            <a href="https://www.cs.cmu.edu/~psodhi/" target="_blank">P. Sodhi</a>, 
            <a href="http://robots.engin.umich.edu/~joshuagm/" target="_blank">J. Mangelson</a>, 
            <a href="https://frc.ri.cmu.edu/~dsw/info/Home.html" target="_blank">D. Wettergreen</a>, and
            <a href="https://www.cs.cmu.edu/~kaess/" target="_blank">M. Kaess</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>IEEE Intl. Conf. on Robotics and Automation, ICRA, 2020. To appear.</em>

            <br>
            <br>
            (<a href="http://www.cs.cmu.edu/~kaess/pub/Suresh20icra.pdf" target="_blank"><b>pdf</b></a> /
            <a href="https://youtu.be/4HgdWJlL8JY" target="_blank"><b>video</b></a>)
             <!-- <p> We present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. The navigation policy reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. Good places to revisit are chosen by a metric of 3D submap saliency. We demonstrate its performance in simulation and real-world experiments.</p> -->
          </td>
        </tr>

        <!-- refr slam -->
        <tr>
          <td width="30%" align="center">
            <img src='data/media/refr_slam/refr2.gif'  width="200" height="100">
            <img src='data/media/refr_slam/refr1.gif'  width="200" height="80">
          </td>
          <td valign="center" width="75%">
            <papertitle>Through-water stereo SLAM with refraction correction for AUV localization
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>S. Suresh</u>,
            <a href="https://www.ri.cmu.edu/ri-people/eric-westman/" target="_blank">E. Westman</a>, and
            <a href="https://www.cs.cmu.edu/~kaess/" target="_blank">M. Kaess</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>IEEE Robotics and Automation Letters, Part of ICRA/RA-L: Presented at ICRA 2019 and published in RA-L</em>, Jan 2019

            <br>
            <br>
            (<a href="http://www.cs.cmu.edu/~kaess/pub/Suresh19ral.pdf" target="_blank"><b>pdf</b></a> /
              <a href="https://youtu.be/fZZTDyLymBs" target="_blank"><b>video</b></a>)
            <!-- <p> High-accuracy, drift-free pose estimates are necessary for inspection tasks in underwater indoor environments, such as nuclear spent pools. We present a novel SLAM formulation using an onboard upward-facing stereo camera for underwater localization. We introduce a refraction correction module modeled after prior work in multimedia photogrammetry. The work is evaluated both simulation and real-world settings.</p> -->
          </td>
        </tr>

        <!-- doe -->
        <tr>
           <td width="25%" align="center">
            <img src='data/media/doe/test.gif'   width="200" height="100">
            <img src='data/media/doe/checker.gif'   width="200" height="80">
          </td>
          <td valign="center" width="75%">
            <papertitle>Localized imaging and mapping for underwater fuel storage basins 
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            J. Hsiung,
            A. Tallaksen,
            L. Papincak,
            <u>S. Suresh</u>,
            H. Jones,
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">W. L. Whittaker</a>, and
            <a href="https://www.cs.cmu.edu/~kaess/" target="_blank">M. Kaess</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>Proceedings of the Symposium on Waste Management, Phoenix, Arizona</em>, Mar 2018

            <br>
            <br>
            (<a href="data/papers/wm18_final.pdf" target="_blank"><b>pdf</b></a> /
              <a href="data/papers/wm18_presentation.pdf" target="_blank"><b>slides</b></a> /
              <a href="https://www.youtube.com/watch?v=R6JUAJq4rE4&feature=youtu.be" target="_blank"><b>video</b></a>)
            <!-- <p> Developed a localized inspection solution for underwater 3D reconstruction of nuclear facilities. The system&mdash;the sensorpod&mdash;consists of a stereo camera, standard <em>and</em> structured light source, IMU and depth sensor. We generate dense reconstructions of underwater targets in tests.</p> -->
          </td>
        </tr>

        <!-- riss -->
        <tr>
           <td width="25%" align="center">
            <img src='data/media/riss/autokrawler.gif'   width="200" height="100">
            <img src='data/media/riss/est.gif'  width="200" height="80">
          </td>
          <td valign="center" width="75%">
            <papertitle>Optical kinematic state estimation of planetary rovers using downward-facing monocular fisheye camera 
            (<a href="data/papers/RISS2016.pdf" target="_blank">pdf</a> /
            <a href="https://youtu.be/-D7WXVTPXuo" target="_blank">video</a> / 
            <a href="https://riss.ri.cmu.edu/wp-content/uploads/2016/09/2016-RISS-Poster-SUDHARSHAN_Suresh.pdf" target="_blank">poster</a>)
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>S. Suresh</u> ,
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/" target="_blank">E. Fang</a>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">W. L. Whittaker</a> 
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>Robotics Institute Summer Scholars Working Paper Journal</em>, Nov 2016
            <br>

            <div style="height:10px;font-size:1px;">&nbsp;</div>
            <papertitle>Camera-Only Kinematics for Small Lunar Rovers
            (<a href="https://www.hou.usra.edu/meetings/leag2016/eposter/5026.pdf" target="_blank">poster</a> /
            <a href="https://www.hou.usra.edu/meetings/leag2016/pdf/5026.pdf" target="_blank">pdf</a>)
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/" target="_blank">E. Fang</a>, 
            <u>S. Suresh</u>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">W. L. Whittaker</a> 
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>Annual Meeting of the Lunar Exploration Analysis Group</em>, Nov 2016
            <!-- <p> We develop a visual state-estimation algorithm for planetary rovers via self-perception. The method uses a single downward-facing fisheye camera to estimate 10-DoF kinematic state on rugged terrain. Our estimation combines fiducial marker tracking, optical flow techniques and knowledge of the rover's kinematic constraints. We demonstrate it performs on the Autokrawler operating in a lunar analogous field test and results agree well with proprioceptive sensors</p> -->
          </td>
        </tr>

        <!-- val -->
        <tr>
          <td width="25%" align="center">
            <img src='data/media/iisc/scanpaths.gif'   width="200"  height="120">
          </td>
          <td valign="center" width="75%">
            <papertitle>Object category understanding via eye fixations on freehand sketches
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://ravika.github.io/index.html" target="_blank">R. K. Sarvadevabhatla</a>,
            <u>S. Suresh</u> and
            <a href="http://cds.iisc.ac.in/faculty/venky/" target="_blank">R. V. Babu</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em>IEEE Transactions on Image Processing (TIP)</em>, May 2017

            <br>
            <br>
            (<a href="https://arxiv.org/pdf/1703.06554.pdf" target="_blank"><b>paper</b></a> /
            <a href="http://val.cds.iisc.ac.in/sketchfix/" target="_blank"><b>project page</b></a>)

            <!-- <p> Research investigates object category understanding and visual saliency for freehand sketches. We create and open-source <a href="http://val.cds.iisc.ac.in/sketchfix/" target="_blank">SketchFix-160</a>, a dataset of free-viewing user tests collected with a monocular eyetracker. We analyse fixation data to reveal multi-level consistency and (a) predict a test sketch's category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects.</p> -->
          </td>
        </tr>
        
        </table>
    
    <br>
    <div style="height:20px;font-size:1px;">&nbsp;</div>

    <!-- Projects -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="100%" valign="middle">
            <h1>Projects</h1>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <!-- Deepgeo-->
          <tr>
            <td width="10%" align="center">
              <img src='data/media/course-projects/deepgeo.gif' width="100" height="100">
            </td>
            <td valign="center" width="90%">
                <papertitle>DeepGeo: photo localization with deep neural network</papertitle> 
                <strong>
                &nbsp;
                (<a href="https://arxiv.org/pdf/1810.03077.pdf"  target="_blank">arXiv</a> / <a href="https://github.com/suddhu/DeepGeo"  target="_blank">github</a>)
                </strong>
                <br>
                <div style="height:3px;font-size:1px;">&nbsp;</div>
                S. Suresh, N. Chodosh, and M. Abello 
                <br>
                <div style="height:5px;font-size:1px;">&nbsp;</div>
                <i>A deep network that beats humans at <a href="https://geoguessr.com/" target="_blank">GeoGuessr</a>, trained on our <i>50States10K</i> dataset.</i>
            </td>
          </tr>
  
          <!-- TAMP -->
          <tr>
            <td width="10%" align="center">
                <img src='data/media/course-projects/tamp.gif' width="100" height="100">
              </td>
            <td valign="center" width="90%">
                <papertitle>Task and motion planning for robotic food preparation</papertitle> 
                &nbsp; 
                <strong>
                (<a href="data/media/course-projects/TAMP4ParfaitsReport.pdf" target="_blank">PDF</a> /
                <a href="https://www.youtube.com/watch?v=VBhGjcgPVqA" target="_blank">Video 1</a> / <a href="https://www.youtube.com/watch?v=BADt_yy_Lvw" target="_blank">Video 2</a>)  
                </strong>
                <br>
                <div style="height:3px;font-size:1px;">&nbsp;</div>
                S. Suresh, T. Rhodes, M. Abello, and H. Yadav
                <br>
                <div style="height:5px;font-size:1px;">&nbsp;</div>
                <i>Hierarchical task and motion planning for a 6-DOF robot arm, to prepare yogurt parfaits!</i>
            </td>
          </tr>
  
          <!-- thin structures -->
          <tr>
              <td width="10%" align="center">
                  <img src='data/media/course-projects/thin.gif' width="100" height="100">
            </td>
            <td valign="center" width="90%">
                <papertitle>Thin structure reconstruction via 3D lines and points 
                  </papertitle> 
                  &nbsp; 
                  <strong>
                  (<a href="data/media/course-projects/LinesPointsSFM.pdf" target="_blank">Poster</a>)
                  </strong>
                  <br>
                  <div style="height:3px;font-size:1px;">&nbsp;</div>
                  S. Suresh and M. Abello
                  <br>
                  <div style="height:5px;font-size:1px;">&nbsp;</div>
                  <i>Reconstructing thin objects in a scene through an SfM pipeline can be hard!</i>
            </td>
          </tr>
  
          <!-- dyn param -->
          <tr>
              <td width="10%" align="center">
                <img src='data/media/course-projects/dynparam.png' width="100">
            </td>
            <td valign="center" width="90%">
                <papertitle>Factor graph optimization for dynamic parameter estimation 
                  </papertitle> 
                  &nbsp; 
                  <strong>
                  (<a href="data/papers/16-711_final.pdf" target="_blank">Report</a>)
                  </strong>
                  <br>
                  <div style="height:3px;font-size:1px;">&nbsp;</div>
                  S. Suresh, E. Dexheimer, and M. Abello
                  <br>
                  <div style="height:5px;font-size:1px;">&nbsp;</div>
                  <i>We implement a method for estimation of MAV poses and dynamic parameters during flight.</i>
            </td>
          </tr>
      </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
        <p align="right"><font size="2" color=#696969>
    Last updated circa May 2020
        <p align="right"><font size="2" color=#696969>
Template stolen from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank"><font size="2"> Jon Barron.</a>
</font></p>

<script type="text/javascript">
  var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
  document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
  try {
    var pageTracker = _gat._getTracker("UA-7580334-1");
    pageTracker._trackPageview();
  } catch (err) {}
</script>
</td></tr>
</table>

</body></html>