<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- remove after done debugging!!!! -->
<meta http-equiv="refresh" content="1" >
<!-- remove after done debugging!!!! -->

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700;
      margin-bottom: 10cm;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
 
     .zero {
      width: 160px;
      height: 80px;
      position: relative;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    mid {
      font-size: 40px;
      position:relative;
      top:2px;
    }


    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" href="https://www.ri.cmu.edu/wp-content/uploads/2017/04/ri-favicon.ico">

  <title>Sudharshan Suresh</title>
  
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Sudharshan Suresh <mid><font color=#F1948A>|</font></mid> <font color=#C0C0C0>Suddhu</font></name>
              <p align="center">
                <font size="3">suddhu [at] cmu [dot] edu </font>
              </p>
              <p><p style = "text-align:justify">I'm a Masters student at the <A href="http://www.ri.cmu.edu/">Robotics Institute</A>, <A href="http://www.cmu.edu"> Carnegie Mellon University</A> working with <A href="http://frc.ri.cmu.edu/~kaess/">Michael Kaess</a> in the <A href="http://rpl.ri.cmu.edu/">Robot Perception Lab</a>. My research interests span visual SLAM and navigation for autonomous robots. Currently, I work on underwater localization for autonomous underwater vehicles (AUVs) and active SLAM for robot exploration.
              </p>
              <p><p style = "text-align:justify">Previously, I worked with <A href="http://www.frc.ri.cmu.edu/users/red/">Red Whittaker</a> on visual state-estimation for lunar rovers. I've also spent time with <A href="http://cds.iisc.ac.in/faculty/venky/">Venkatesh Babu</a> at <A href="https://www.iisc.ac.in/">IISc</a>, working on visual understanding. I majored in Controls and Instrumentation at <A href="https://www.nitt.edu/">NIT Trichy</a>. 
              </p>
              <p align=center>
                <a href="misc/CV_Suddhu.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=xYC738YAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sudharshansuresh/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://www.ri.cmu.edu/ri-people/sudharshan-suresh/"> RI webpage </a>
              </p>
            </td>
            <td width="50%">
              <img src="misc/suddhu-heasdshot.jpg">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
            </td>
          </tr>
        </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <!-- current -->
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" bgcolor="#FFFFE0">
          <td width="25%">
            <div class="zero">
              <div class="two" id='aperture_image'><img src='misc/suddhu-headshot.jpg'></div>
              <img src='misc/suddhu-headshot.jpg'>
            </div>
            <script type="text/javascript">
              function aperture_start() {
                document.getElementById('aperture_image').style.opacity = "1";
              }

              function aperture_stop() {
                document.getElementById('aperture_image').style.opacity = "0";
              }
              aperture_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <papertitle><font color=#FF8080><strong>(ongoing)</strong></font> Planning Revisits in Exploration for Saliency-Aware active SLAM Underwater
            </papertitle>
            <p> Can we harness visual data to identify waypoints for loop closure in an underwater exploration task? This research stems from previous work in active SLAM for the coverage problem. Image saliency metrics give us good candidates for loop closure, enabling drift-free operation and accurate underwater sonar maps.</p>
          </td>
        </tr>

        <!-- refr slam -->
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
          <td width="25%">
            <div class="one">
              <div class="two" id='aperture_image'><img src='images/aperture_after.jpg'></div>
              <img src='images/aperture_before.jpg'>
            </div>
            <script type="text/javascript">
              function aperture_start() {
                document.getElementById('aperture_image').style.opacity = "1";
              }

              function aperture_stop() {
                document.getElementById('aperture_image').style.opacity = "0";
              }
              aperture_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <papertitle>Through-water Stereo SLAM with Refraction Correction for AUV Localization
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>Sudharshan Suresh</u>,
            <a href="https://www.ri.cmu.edu/ri-people/eric-westman/">Eric Westman</a> and
            <a href="http://frc.ri.cmu.edu/~kaess/">Michael Kaess</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em><font color=#FF8080><strong>under review</strong></font> in IEEE Robotics and Automation Letters (ICRA/RAL)</em>, 2019
            <br>
            <strong>
              <a href="data/papers/RAL_ICRA2019.pdf">pre-print</a> /
              <a href="https://youtu.be/fZZTDyLymBs">video</a>
            </strong>
            <p> High-accuracy, drift-free pose estimates are
                necessary for inspection tasks in underwater indoor environments, such as nuclear spent pools. We present a novel SLAM formulation using an onboard upward-facing stereo camera for underwater localization. We introduce a refraction correction module modeled after prior work in multimedia photogrammetry. The work is evaluated both simulation and real-world settings.</p>
          </td>
        </tr>

        <!-- doe -->
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
          <td width="25%">
            <div class="one">
              <div class="two" id='aperture_image'><img src='images/aperture_after.jpg'></div>
              <img src='images/aperture_before.jpg'>
            </div>
            <script type="text/javascript">
              function aperture_start() {
                document.getElementById('aperture_image').style.opacity = "1";
              }

              function aperture_stop() {
                document.getElementById('aperture_image').style.opacity = "0";
              }
              aperture_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <papertitle>Localized Imaging and Mapping for Underwater Fuel Storage Basins
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            Jerry Hsiung,
            Andrew Tallaksen,
            Lawrence Papincak,
            <u>Sudharshan Suresh</u>,<br>
            Heather Jones,
            <a href="http://www.frc.ri.cmu.edu/users/red/">William L. "Red" Whittaker</a> and
            <a href="http://frc.ri.cmu.edu/~kaess/">Michael Kaess</a>
            <br>
            <em>Proceedings of the Symposium on Waste Management, Phoenix, Arizona</em>, Mar 2018
            <br>
            <strong>
              <a href="data/papers/wm18_final.pdf">pdf</a> /
              <a href="data/papers/wm18_presentation.pdf">slides</a> /
              <a href="https://youtu.be/fZZTDyLymBs">video</a>
            </strong>
            <p> Developed a localized inspection solution for underwater 3D reconstruction of nuclear facilities. The system&mdash;the sensorpod&mdash;consists of a stereo camera, standard <em>and</em> structured light source, IMU and depth sensor. We generate dense reconstructions of underwater targets in tests.</p>
          </td>
        </tr>

        <!-- val -->
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
          <td width="25%">
            <div class="one">
              <div class="two" id='aperture_image'><img src='images/aperture_after.jpg'></div>
              <img src='images/aperture_before.jpg'>
            </div>
            <script type="text/javascript">
              function aperture_start() {
                document.getElementById('aperture_image').style.opacity = "1";
              }

              function aperture_stop() {
                document.getElementById('aperture_image').style.opacity = "0";
              }
              aperture_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <papertitle>Object Category Understanding via Eye Fixations on Freehand Sketches
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://ravika.github.io/index.html">Ravi Kiran Sarvadevabhatla</a>,
            <u>Sudharshan Suresh</u> and
            <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
            <br>
            <em>IEEE Transactions on Image Processing (TIP)</em>, May 2017
            <br>
            <strong>
              <a href="https://ieeexplore.ieee.org/document/7866001">paper</a> /
              <a href="http://val.cds.iisc.ac.in/sketchfix/">project page</a>
            </strong>
            <p> Research investigates object category understanding and visual saliency for freehand sketches. We create and open-source <a href="http://val.cds.iisc.ac.in/sketchfix/">SketchFix-160</a>, a dataset of free-viewing user tests collected with a monocular eyetracker. We analyse fixation data to reveal multi-level consistency and (a) predict a test sketch's category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects.</p>
          </td>
        </tr>

        <!-- riss -->
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
          <td width="25%">
            <div class="one">
              <div class="two" id='aperture_image'><img src='images/aperture_after.jpg'></div>
              <img src='images/aperture_before.jpg'>
            </div>
            <script type="text/javascript">
              function aperture_start() {
                document.getElementById('aperture_image').style.opacity = "1";
              }

              function aperture_stop() {
                document.getElementById('aperture_image').style.opacity = "0";
              }
              aperture_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <papertitle>Optical Kinematic State Estimation of Planetary Rovers using Downward-Facing Monocular Fisheye Camera
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>Sudharshan Suresh</u> ,
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/">Eugene Fang</a>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/">William L. "Red" Whittaker</a> 
            <br>
            <em>Robotics Institute Summer Scholars Working Paper Journal</em>, Nov 2016
            <br>
            <strong>
              <a href="data/papers/RISS2016.pdf">pdf</a> /
              <a href="https://youtu.be/-D7WXVTPXuo">video</a> / 
              <a href="https://riss.ri.cmu.edu/wp-content/uploads/2016/09/2016-RISS-Poster-SUDHARSHAN_Suresh.pdf">poster</a>
            </strong>

            <div style="height:10px;font-size:1px;">&nbsp;</div>
            <papertitle>Camera-Only Kinematics for Small Lunar Rovers
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/">Eugene Fang</a>, 
            <u>Sudharshan Suresh</u>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/">William L. "Red" Whittaker</a> 
            <br>
            <em>Annual Meeting of the Lunar Exploration Analysis Group</em>, Nov 2016
            <br>
            <strong>
              <a href="https://www.hou.usra.edu/meetings/leag2016/eposter/5026.pdf">poster</a> /
              <a href="https://www.hou.usra.edu/meetings/leag2016/pdf/5026.pdf">pdf</a>
            </strong>
            <p> We develop a visual state-estimation algorithm for planetary rovers via self-perception. The method uses a single downward-facing fisheye camera to estimate 10-DoF kinematic state on rugged terrain. Our estimation combines fiducial marker tracking, optical flow techniques and knowledge of the rover's kinematic constraints. We demonstrate it performs on the Autokrawler operating in a lunar analogous field test and results agree well with proprioceptive sensors</p>
          </td>
        </tr>

        </table>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
  var pageTracker = _gat._getTracker("UA-xxxxxx-x");
  pageTracker._trackPageview();
} catch(err) {}
</script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>
        <p align="right"><font size="2">
    Last updated: November 2018
        <p align="right"><font size="2">
Webpage adapted from <a href="http://www.cs.berkeley.edu/~barron/">Jon Barron.</a>
</font></p>

</td></tr>
</table>

</body></html>