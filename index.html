<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- remove after done debugging!!!! -->
<!-- <meta http-equiv="refresh" content="1" > -->
<!-- remove after done debugging!!!! -->

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 30px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700;
      margin-bottom: 10cm;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
 
     .zero {
      width: 160px;
      height: 80px;
      position: relative;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    mid {
      font-size: 40px;
      position:relative;
      top:2px;
    }


    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" href="https://www.ri.cmu.edu/wp-content/uploads/2017/04/ri-favicon.ico">

  <title>Sudharshan Suresh</title>
  
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
</head>
<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="75%" valign="middle">
              <p align="center">
                <name>Sudharshan Suresh <mid><font color=#FF0000>|</font></mid> <font color=#C0C0C0>Suddhu</font></name>
              <p align="center">
                <font size="3">suddhu <font color=#FF0000>[at]</font> cmu <font color=#FF0000>[dot]</font> edu </font>
              </p>
              <p><p style = "text-align:justify">I'm a Masters student at the <A href="http://www.ri.cmu.edu/" target="_blank">Robotics Institute</A>, <A href="http://www.cmu.edu" target="_blank"> Carnegie Mellon University</A> working with <A href="http://frc.ri.cmu.edu/~kaess/" target="_blank">Michael Kaess</a> in the <A href="http://rpl.ri.cmu.edu/" target="_blank">Robot Perception Lab</a>. My research interests span visual SLAM and navigation for autonomous robots. Currently, I work on underwater localization for autonomous underwater vehicles (AUVs) and active SLAM for robot exploration.
              </p>
              <p><p style = "text-align:justify">Previously, I worked with <A href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">Red Whittaker</a> on visual state-estimation for lunar rovers. I've also spent time with <A href="http://cds.iisc.ac.in/faculty/venky/" target="_blank">Venkatesh Babu</a> at <A href="https://www.iisc.ac.in/">IISc</a>, working on visual understanding. In my days of yore, I majored in Controls and Instrumentation at <A href="https://www.nitt.edu/" target="_blank">NIT Trichy</a>. 
              </p>
              <p align=center>
                <strong>
                <a href="misc/CV_Suddhu.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=xYC738YAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sudharshansuresh/" target="_blank"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://www.ri.cmu.edu/ri-people/sudharshan-suresh/" target="_blank"> RI webpage </a>
                </strong>
              </p>
            </td>
            <td width="50%">
              <img src="misc/suddhu-headshot.jpg" style="width: 200; height: auto">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
            </td>
          </tr>
        </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <!-- current -->
        <tr bgcolor="#FFFFE0">
          <td width="8%">          
              <img src='data/media/active_sal/sal_test.gif' width="150">
          </td>
          <td width="8%">    
              <img src='data/media/active_sal/schematic.png' width="150">      
          </td>
          <td valign="top" width="80%">
            <papertitle><font color=#FF0000><strong>(ongoing)</strong></font> Planning Revisits in Exploration for Saliency-Aware active SLAM Underwater
            </papertitle>
            <p> Can we harness visual data to identify waypoints for loop closure in an underwater exploration task? This research stems from previous work in active SLAM for the coverage problem. Image saliency metrics give us good candidates for loop closure, enabling drift-free operation and accurate underwater sonar maps.</p>
          </td>
        </tr>


      </table>
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- refr slam -->
        <br>
        <tr>
          <td width="25%" align="center">
            <img src='data/media/refr_slam/refr2.gif' >
            <img src='data/media/refr_slam/refr1.gif' >
          </td>
          <td valign="top" width="75%">
            <papertitle>Through-water Stereo SLAM with Refraction Correction for AUV Localization
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>Sudharshan Suresh</u>,
            <a href="https://www.ri.cmu.edu/ri-people/eric-westman/" target="_blank">Eric Westman</a> and
            <a href="http://frc.ri.cmu.edu/~kaess/" target="_blank">Michael Kaess</a>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <em><font color=#FF0000><strong>Accepted, to appear</strong></font> in IEEE Robotics and Automation Letters (ICRA/RA-L)</em>, 2019
            <br>
            <strong>
              <a href="data/papers/RAL_ICRA2019.pdf" target="_blank">pre-print</a> /
              <a href="https://youtu.be/fZZTDyLymBs" target="_blank">video</a>
            </strong>
            <p> High-accuracy, drift-free pose estimates are
                necessary for inspection tasks in underwater indoor environments, such as nuclear spent pools. We present a novel SLAM formulation using an onboard upward-facing stereo camera for underwater localization. We introduce a refraction correction module modeled after prior work in multimedia photogrammetry. The work is evaluated both simulation and real-world settings.</p>
          </td>
        </tr>

        <!-- doe -->
        <tr>
           <td width="25%" align="center">
            <img src='data/media/doe/test.gif' >
            <img src='data/media/doe/checker.gif' >
          </td>
          <td valign="top" width="75%">
            <papertitle>Localized Imaging and Mapping for Underwater Fuel Storage Basins
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            Jerry Hsiung,
            Andrew Tallaksen,
            Lawrence Papincak,
            <u>Sudharshan Suresh</u>,<br>
            Heather Jones,
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">William L. "Red" Whittaker</a> and
            <a href="http://frc.ri.cmu.edu/~kaess/" target="_blank">Michael Kaess</a>
            <br>
            <em>Proceedings of the Symposium on Waste Management, Phoenix, Arizona</em>, Mar 2018
            <br>
            <strong>
              <a href="data/papers/wm18_final.pdf" target="_blank">pdf</a> /
              <a href="data/papers/wm18_presentation.pdf" target="_blank">slides</a> /
              <a href="https://www.youtube.com/watch?v=R6JUAJq4rE4&feature=youtu.be" target="_blank">video</a>
            </strong>
            <p> Developed a localized inspection solution for underwater 3D reconstruction of nuclear facilities. The system&mdash;the sensorpod&mdash;consists of a stereo camera, standard <em>and</em> structured light source, IMU and depth sensor. We generate dense reconstructions of underwater targets in tests.</p>
          </td>
        </tr>

        <!-- riss -->
        <tr>
           <td width="25%" align="center">
            <img src='data/media/riss/autokrawler.gif' >
            <img src='data/media/riss/est.gif' >
          </td>
          <td valign="top" width="75%">
            <papertitle>Optical Kinematic State Estimation of Planetary Rovers using Downward-Facing Monocular Fisheye Camera
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <u>Sudharshan Suresh</u> ,
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/" target="_blank">Eugene Fang</a>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">William L. "Red" Whittaker</a> 
            <br>
            <em>Robotics Institute Summer Scholars Working Paper Journal</em>, Nov 2016
            <br>
            <strong>
              <a href="data/papers/RISS2016.pdf" target="_blank">pdf</a> /
              <a href="https://youtu.be/-D7WXVTPXuo" target="_blank">video</a> / 
              <a href="https://riss.ri.cmu.edu/wp-content/uploads/2016/09/2016-RISS-Poster-SUDHARSHAN_Suresh.pdf" target="_blank">poster</a>
            </strong>

            <div style="height:10px;font-size:1px;">&nbsp;</div>
            <papertitle>Camera-Only Kinematics for Small Lunar Rovers
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://www.ri.cmu.edu/ri-people/eugene-fang/" target="_blank">Eugene Fang</a>, 
            <u>Sudharshan Suresh</u>,
            and
            <a href="http://www.frc.ri.cmu.edu/users/red/" target="_blank">William L. "Red" Whittaker</a> 
            <br>
            <em>Annual Meeting of the Lunar Exploration Analysis Group</em>, Nov 2016
            <br>
            <strong>
              <a href="https://www.hou.usra.edu/meetings/leag2016/eposter/5026.pdf" target="_blank">poster</a> /
              <a href="https://www.hou.usra.edu/meetings/leag2016/pdf/5026.pdf" target="_blank">pdf</a>
            </strong>
            <p> We develop a visual state-estimation algorithm for planetary rovers via self-perception. The method uses a single downward-facing fisheye camera to estimate 10-DoF kinematic state on rugged terrain. Our estimation combines fiducial marker tracking, optical flow techniques and knowledge of the rover's kinematic constraints. We demonstrate it performs on the Autokrawler operating in a lunar analogous field test and results agree well with proprioceptive sensors</p>
          </td>
        </tr>

        <!-- val -->
        <tr>
          <td width="25%" align="center">
            <img src='data/media/iisc/scanpaths.gif' >
          </td>
          <td valign="top" width="75%">
            <papertitle>Object Category Understanding via Eye Fixations on Freehand Sketches
            </papertitle>
            <div style="height:5px;font-size:1px;">&nbsp;</div>
            <a href="https://ravika.github.io/index.html" target="_blank">Ravi Kiran Sarvadevabhatla</a>,
            <u>Sudharshan Suresh</u> and
            <a href="http://cds.iisc.ac.in/faculty/venky/" target="_blank">R. Venkatesh Babu</a>
            <br>
            <em>IEEE Transactions on Image Processing (TIP)</em>, May 2017
            <br>
            <strong>
              <a href="https://ieeexplore.ieee.org/document/7866001" target="_blank">paper</a> /
              <a href="http://val.cds.iisc.ac.in/sketchfix/" target="_blank">project page</a>
            </strong>
            <p> Research investigates object category understanding and visual saliency for freehand sketches. We create and open-source <a href="http://val.cds.iisc.ac.in/sketchfix/" target="_blank">SketchFix-160</a>, a dataset of free-viewing user tests collected with a monocular eyetracker. We analyse fixation data to reveal multi-level consistency and (a) predict a test sketch's category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects.</p>
          </td>
        </tr>
        
        </table>

        <table width="100%" align="center" border="0" cellspacing="2" cellpadding="10">
        <tr>
          <td width="100%" valign="middle">
            <heading>Projects</heading>
            
          </td>
        </tr>

        <tr>

        <td valign="top" width="100%">
        <papertitle>DeepGeo: Photo Localization with Deep Neural Network</papertitle> 
        <strong>
        &nbsp; &nbsp;
        [<a href="https://arxiv.org/abs/1810.03077">arXiv</a>]
        </strong>
        <br>
        Sudharshan Suresh, Nathaniel Chodosh and Montiel Abello 
        <br>
        <i>A deep network that beats humans at <a href="https://geoguessr.com/">GeoGuessr</a>, trained on our <i>50States10K</i> dataset.</i>
        </td>
        </tr>   

        <tr>
        <td valign="top" width="100%">
        <papertitle>Task and Motion Planning for Robotic Food Preparation</papertitle> 
        &nbsp; &nbsp;
        <strong>
        [<a href="data/media/course-projects/TAMP4ParfaitsReport.pdf">PDF</a> /
        <a href="https://www.youtube.com/watch?v=VBhGjcgPVqA">Video 1</a> / <a href="https://www.youtube.com/watch?v=BADt_yy_Lvw">Video 2</a>]    
        </strong>
        <br>
        Sudharshan Suresh, Travers Rhodes, Montiel Abello and Himanshi Yadav
        <br>
        <i>Hierarchical task and motion planning for a 6-DOF robot arm, to prepare yogurt parfaits!</i>
        </td>
        </tr>   
   
        <td valign="top" width="100%">
        <papertitle>Thin Structure Reconstruction via 3D Lines and Points 
        </papertitle> 
        &nbsp; &nbsp;
        <strong>
        [<a href="data/media/course-projects/LinesPointsSFM.pdf">Poster</a>]
        </strong>
        <br>
        Sudharshan Suresh and Montiel Abello
        <br>
        <i>We combine edge data and sparse features in the SfM pipeline to recover thin objects in a scene.</i>
        </td>
        </tr>   
        </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>
        <p align="right"><font size="1" color=#696969>
    Last updated circa Dec 2018
        <p align="right"><font size="1" color=#696969>
Template begged, borrowed or stolen from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank"><font size="1"> Jon Barron.</a>
</font></p>

</td></tr>
</table>

</body></html>