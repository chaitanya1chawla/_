<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Multi-View Stereo by Temporal Nonparametric Fusion</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="6a77a015-92e1-4efa-807f-a27bc057f89f" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">📹</span></div><h1 class="page-title">Multi-View Stereo by Temporal Nonparametric Fusion</h1></header><div class="page-body"><p id="b071ee81-0257-42e9-8805-8598ecd92fec" class=""><a href="https://aaltoml.github.io/GP-MVS/">https://aaltoml.github.io/GP-MVS/</a></p><p id="c45b0e2a-a2ae-4553-b6bc-2a9da1d22428" class="">Problem: Accurate, real-time dense depth estimation from monocular sequence </p><p id="89b48328-079d-4904-88b2-19df58383370" class="">Idea: Similar views should have similar representations in latent-space</p><p id="36020c83-900a-4e08-8142-aeb8e1619be7" class="">Solution: Soft-constrain bottleneck layer of depth estimation network using Gaussian Procceses (GPs) with appropriate pose-kernel</p><p id="084368ba-0ecd-4c51-86e7-955ed6a3557b" class="">
</p><p id="e5fb84d0-6a30-4f62-8848-a7db19aa04d6" class="">First, we go into the problem of estimating depth for a single view (using multiview constraints in this case)</p><h1 id="ea197c98-125f-4327-a08c-33364e4fd8f8" class="">Network Architecture for Single Frame Depth Estimation</h1><p id="d4dd9b19-bf72-465e-a045-37970435bc43" class="">Exact same architecture as in MVDepthNet: Real-time Multiview Depth Estimation Neural Network</p><p id="b7950ca4-bcf5-4e1f-a0e1-69ccab852123" class="">
</p><p id="e2a8f305-8120-47af-84d1-c92069e5034e" class="">Problem: Leverage multiview geometry in real-time dense depth network 
(assuming known poses of images from odometry source)</p><p id="6ed7cb94-2411-4679-a18e-1d2a17187a25" class="">Idea: Cost volumes can directly encode geometric constraints so that networks don&#x27;t have to</p><p id="7f0b93fc-8463-4dc1-86dc-bc52e1b58e23" class="">Solution: Feed cost volume along with input image into network</p><p id="8b372313-05da-461c-800c-f21ed1fe0191" class="">
</p><p id="91d53b76-bdb6-4f73-9121-ac3f4b2d1680" class="">Methods prior to this work:</p><ul id="c48d96fc-87c9-4d22-9591-c43942d6ad66" class="bulleted-list"><li>Single-view methods do not generalize well due to lack of constraints</li></ul><ul id="770f90d8-3c8d-4fc7-895a-36781ff43e3d" class="bulleted-list"><li>Difficult to parameterize epipolar geometry into network</li></ul><ul id="a1cb6259-8c46-433b-bd5b-3141daf43f47" class="bulleted-list"><li>Cannot easily augment dataset due to multiview constraints</li></ul><h3 id="6f758e2b-8f59-4b42-bd63-1df904eb273f" class="">Cost Volume Construction</h3><ul id="91f02eef-0175-428c-93d2-78a2b1dcda96" class="bulleted-list"><li>Construct cost-volume with respect to a reference frame and auxiliary frames</li></ul><ul id="57885664-89ea-431c-8aa5-64ea5ce9d429" class="bulleted-list"><li>Discretize inverse depth (here using 64 values)</li></ul><figure id="0a440300-b285-4ed6-b14c-15e491089ef0" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled.png"/></a></figure><ul id="a307feb7-4c4b-457c-b909-5a91192546ee" class="bulleted-list"><li>Note: No window size used for cost aggregation (only single pixel used) so that details are preserved</li></ul><p id="349c8527-91e9-4f65-829d-216321f05098" class="">
</p><h3 id="025aeb99-ef6e-4840-b26d-82cee677fd55" class="">Network Architecture</h3><p id="2caf817d-83fe-483f-9eae-0ae3ea4c66d3" class="">Feed RGB reference frame (depth 3) and cost volume (depth 64) into encoder-decoder network with skip connections</p><figure id="8894a46a-4e39-46cc-b553-3dad11113393" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%201.png"><img style="width:2230px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%201.png"/></a></figure><p id="dcf77de6-9d0e-4cb4-b9c2-ae978328a27b" class="">Data augmentation is now straightforward because all inputs and ground-truth are with respect to the same frame</p><p id="e4346666-ef3a-4844-89b7-dbe29b3e2eb6" class="">
</p><h3 id="f8731107-f868-4ec9-9780-9aadc090bf78" class="">Notable Ablation Studies</h3><p id="f6893da1-d18c-4a4b-a90c-2f17a8cbff1c" class="">Example of how adding RGB image to inputs provides finer details as compared to cost-volume alone:</p><figure id="638eb903-9014-454d-95fd-d9d54821815c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%202.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%202.png"/></a></figure><p id="9b791964-edba-41ca-b1bd-142c259eb92b" class="">
</p><p id="77cc110f-1469-4348-bf2a-47e477bba889" class="">Impact of number of auxiliary frames on accuracy of depth estimation:</p><figure id="e340ff48-8ce9-4f54-9468-4e90749c7486" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%203.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%203.png"/></a></figure><p id="9c2a2eaf-c9f4-4704-b6e7-6f40cb4013f4" class="">
</p><h3 id="6505e302-c466-4dc4-bc09-7935eda4fb6a" class="">Problems</h3><ul id="972d3b19-bc6e-4fbf-99d0-34aded8584e4" class="bulleted-list"><li>While MVDepthNet uses a window of frames, it cannot leverage information beyond this window</li></ul><ul id="5f7dcb3b-0dad-4029-a3eb-2c88e7687e0d" class="bulleted-list"><li>If places revisited, will attempt independent estimation of depth and lack consistency</li></ul><ul id="488132cc-711b-4a98-9842-37f1be03710c" class="bulleted-list"><li>To address this issue, latent representations can enforce temporal consistency between different frames</li></ul><h1 id="af8bc4f0-a4b8-4042-99d2-21b9e4999923" class="">Pose-Kernel Gaussian Process Prior</h1><h3 id="89f771be-c73b-436e-b70b-7363da1aabb7" class="">Gaussian Process Reminder</h3><ul id="b7332b5d-1096-4c98-92cb-caf553b403e9" class="bulleted-list"><li>Nonparametric method often used for regression</li></ul><ul id="657068d3-26a6-4886-8002-3a6798f39074" class="bulleted-list"><li>Observed function values can be used to infer posterior distribution over functions</li></ul><ul id="0e8639e1-24ec-4cfe-bec0-0767350e9e14" class="bulleted-list"><li>Condition on training data to retrieve predictions for test data</li></ul><p id="0737a837-80cc-453e-b522-cd268f16b1c0" class="">
</p><p id="d069cdf4-d454-40a0-8c4a-046d7f645540" class="">Figure generated using <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">https://distill.pub/2019/visual-exploration-gaussian-processes/</a></p><figure id="8497a035-be43-4a89-a913-b77aad876076" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%204.png"><img style="width:816px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%204.png"/></a></figure><h3 id="603a39af-d156-4111-8936-42041917a316" class="">Pose Kernel</h3><ul id="3fcbfc23-cb1f-45a0-ad2e-2df25999f91b" class="bulleted-list"><li>For kernel to assess similarity between poses, we need some a distance metric between rigid-body poses:</li></ul><figure id="e88d3920-5f0f-4592-92ac-587082820084" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%205.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%205.png"/></a></figure><ul id="bad6249b-e40b-4493-bccf-697f5f9903f3" class="bulleted-list"><li>Matern kernel used:</li></ul><figure id="e0e2a14c-30d7-4729-9b10-aab2ef0dbbb7" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%206.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%206.png"/></a></figure><ul id="96092137-a85f-4ee2-a202-70d2956bc1bd" class="bulleted-list"><li>Two learnable parameters here: amplitude <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span></span><span>﻿</span></span> and length-scale <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">ℓ</mi></mrow><annotation encoding="application/x-tex">\ell</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">ℓ</span></span></span></span></span><span>﻿</span></span>.</li></ul><p id="90568ab3-e668-48d5-86bd-648b7cb87109" class="">
</p><h3 id="8c02414c-0e5f-4ecf-8103-b4e2d0f16bc4" class="">Gaussian Process Formulation</h3><ul id="70a1a8c4-8694-4f69-a32d-d8ab31a00a0b" class="bulleted-list"><li>Independent GP priors are placed on all latent space values <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">z_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> for <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2...</mn><mo stretchy="false">(</mo><mn>512</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>10</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">j = 1,2...(512 \times 8 \times 10)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></li></ul><figure id="bc3951e9-ce73-4699-9b16-4c38d7acf62c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%207.png"><img style="width:432px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%207.png"/></a></figure><ul id="9f4d90c9-2209-4bf5-b8fe-5fb69443ef5d" class="bulleted-list"><li>The third learnable GP parameter is <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span></span><span>﻿</span></span>, which is the noise standard deviation.  </li></ul><p id="8a8b6a0d-d00a-4007-afc9-11544ef3da1f" class="">
</p><p id="d4df7d34-d4f9-421a-bbc4-328dc686b950" class="">Graphical model overview (online method):</p><figure id="bbc902dd-3c9e-472d-bb77-82fc4a790c25" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%208.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%208.png"/></a></figure><h1 id="d3e97e1e-6471-4619-bfae-8e3ce6afe81b" class="">Estimation</h1><p id="6f80cc02-006a-4d08-8f69-22be0154ebae" class="">Batch method can account for relationships between all poses while online method only has linear complexity</p><h3 id="7a154a11-4502-44aa-b691-84c17474a4df" class="">Batch (Offline)</h3><p id="23d5fe26-ed5c-4866-90f1-e94c8d3280f8" class="">
</p><figure id="1ce557c6-b6a9-4fa3-961f-152c9c3bfc3d" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%209.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%209.png"/></a></figure><ul id="b97ef174-b348-43a5-9ec9-a456f43eb299" class="bulleted-list"><li>Solve GP for all N frames</li></ul><figure id="25929697-5fbb-4d13-ad0e-e8be69d495db" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2010.png"><img style="width:576px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2010.png"/></a></figure><ul id="e7322d99-6ca3-4036-af4e-4bf7accc9b73" class="bulleted-list"><li>Efficient because matrix <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">C</span></span></span></span></span></span><span>﻿</span></span> only depends on kernel, which only depends on poses, so inverse can be shared across all <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">512 \times 8 \times 10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span></span><span>﻿</span></span> subproblems.</li></ul><ul id="9a301f43-109a-4e08-ac55-1cdea47a2bd2" class="bulleted-list"><li>Posterior mean passed through decoder to retrieve the depth map</li></ul><ul id="9718b21d-4be3-475e-abf0-7acf2d8c5419" class="bulleted-list"><li>Matrix inversion limited by number of poses, becomes too expensive after hundreds of frames</li></ul><h3 id="25b28d98-b839-4be7-8fa1-2abc8154117e" class="">Online</h3><figure id="6c4443bb-5a63-48ba-91f4-0c1539154d09" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2011.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2011.png"/></a></figure><ul id="ec3ee30e-bf0c-4f6d-ae27-c8c8305f4046" class="bulleted-list"><li>Relax GP graphical model to be a Markov chain</li></ul><ul id="1458e674-c919-4a02-bba8-c6b70413355b" class="bulleted-list"><li>Reduce the complexity of GP from <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> to <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>, by reformulating it as a Kalman filtering problem</li></ul><ul id="77ab3a40-5188-4f04-b16d-68fe74db82f4" class="bulleted-list"><li>Convert the problem to a state-space model</li></ul><ul id="a3c37c4d-8dad-4c3e-a790-e5b3288bb723" class="bulleted-list"><li>Latent space encoding conditioned on all image-pose pairs up until current pair</li></ul><h1 id="0c3b8ef6-8abf-4bfb-aaa7-0cb798053a42" class="">Experiments</h1><p id="d3462245-4c61-4390-be2b-c8a1d46223ce" class="">Generally results in improved results</p><figure id="be37d8cd-8b86-43b2-9068-c16ca31404a2" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2012.png"><img style="width:2100px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2012.png"/></a></figure><p id="685dedc2-9f3f-424e-b553-5b93df71f5e8" class="">
</p><p id="66600522-66ff-4a8c-8496-094d3aa67575" class="">Mistakes made early in online method can take more observations to correct since they are propagated forward</p><figure id="24f99ebc-a856-463a-924d-58ba511ae3db" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2013.png"><img style="width:1924px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2013.png"/></a></figure><p id="e021d457-1b8d-491c-a55b-395473b8cfc7" class="">
</p><p id="816c73c9-ac75-4a5a-89c9-f83a5cd65015" class="">Comparison of different kernels on results</p><figure id="8fab9fa6-b75a-4f91-a098-d0d55a232477" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2014.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2014.png"/></a></figure><p id="83a9f267-216c-4c94-92d4-83deef89a412" class="">
</p><p id="34cf40be-1310-430c-bed5-e4c751f5cf6f" class="">This method with 2 frames outperforms other methods with 5 frames</p><figure id="7b933e04-b2ba-4f54-963a-731eb93b6b7c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2015.png"><img style="width:912px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2015.png"/></a></figure><h1 id="5e1e1745-1e9c-49eb-b3c9-5a687114e5b2" class="">Discussion</h1><h3 id="7bc484f3-c748-406d-8e83-3c4eb500ac20" class="">Online Method Limitations</h3><ul id="58d33fad-334d-4e7a-b3cb-ed62a0818d62" class="bulleted-list"><li>No mention of offline runtime?</li></ul><ul id="aa5de211-e981-4fee-b85f-9ce9cbe4539a" class="bulleted-list"><li>Switching to online method loses many of the benefits proposed by this formulation (online method has an implicit window-size)</li></ul><p id="6e244e36-c594-4143-8831-74857da977c6" class="">
</p><h3 id="9e594396-21c8-4684-b734-ea9a7d94f718" class="">Generalization</h3><ul id="55d178a1-c2ff-4b73-a983-b576f1b05770" class="bulleted-list"><li>While very efficient and provides improvement in indoor scenes, generalization should be discussed further</li></ul><p id="49e85ff3-9802-4c7c-9345-2d703ef6d99b" class="">
</p><ul id="e380397b-7993-4ea3-9d8b-c996608497d2" class="bulleted-list"><li>Does this method help for some motions more than others? (Rotation vs. translation)<ul id="3e2d36e6-e11f-45d0-8980-85d590c7da29" class="bulleted-list"><li>MVDepthNet cost volume will lack information for pure rotation?</li></ul><ul id="221503bc-5fe9-4a97-9807-64e810a7a626" class="bulleted-list"><li>This method can  propagate future information back into past, while MVDepthNet cannot</li></ul></li></ul><p id="82ba91d9-56af-4804-90af-64dbdc08d385" class="">
</p><ul id="1b3f3520-7155-43bb-b082-313d26c55dca" class="bulleted-list"><li>Gaussian Process lacks information about size of scene<ul id="77d67460-533b-45c8-a94f-1b32e5e5a309" class="bulleted-list"><li>Length-scale parameter learned, but may not generalize to larger scenes</li></ul><ul id="0269bfd8-f1c4-4e78-b6ff-cc3283f72cf9" class="bulleted-list"><li>Influence of poses within 1m very different depending on indoor vs. outdoor scene</li></ul><ul id="ae00402c-47ea-4b7e-a381-bbad91407a99" class="bulleted-list"><li>Kernels could be learned, but pose information alone may not be sufficient</li></ul></li></ul><p id="73fd327c-f695-4ac1-aa83-6b1de2941dcb" class="">
</p><ul id="24970e8a-c50b-4ef6-b9ef-7e57ab97a129" class="bulleted-list"><li>Computation may be severely limited if more information was included, from Section 3.3:<ul id="73180714-b7ce-4d2a-97c9-fc5a070ce95b" class="bulleted-list"><li>&quot;Because the likelihood is Gaussian and all the GPs share the same poses at which the covariance function is evaluated,we may solve all the 512×8×10GP regression problems with one matrix inversion. This is due to the posterior co-variance only being a function of the input poses, not values of learnt representations of images (i.e.,y does not appear in the posterior variance terms in Eq. 5)&quot;</li></ul></li></ul><h3 id="1c7208d3-759c-4976-af5b-d9e8eb01f15e" class="">Graphical Models and Refinement of Latent Space</h3><p id="20649cef-1ca5-46c3-9169-de395ca70fab" class="">Interesting to think about ways of utilizing latent space in sequential estimation</p><ul id="f553dd28-6920-4671-b267-d537131d6ae6" class="bulleted-list"><li>CodeSLAM propagates observed errors to refine latent space
(geometric and photometric errors should be minimized)</li></ul><figure id="3423980b-9a68-4833-a562-ec80c3f55368" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2016.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2016.png"/></a></figure><ul id="98aa9eec-f30b-41c1-9161-e94b3f2cfc0c" class="bulleted-list"><li>This work implicitly constrains the latent space according to hand-crafted kernel
(similar poses should have similar latent spaces)</li></ul><p id="1262913d-a77b-49aa-b93d-3a1cdb74eadc" class="">
</p><h3 id="c6af69d0-811f-4e0a-b40e-30b5b7c4c61e" class="">Uncertainty Estimation</h3><ul id="93bc0e9e-e1b3-4b25-9e1d-136912b8069c" class="bulleted-list"><li>Uncertainty estimation especially useful for the downstream tasks we are interested in</li></ul><p id="ce987cea-4a9a-48c6-be99-509b9fe73089" class="">
</p><ul id="c561508d-7909-4a1f-82f2-05bcf09a38c8" class="bulleted-list"><li>Is the uncertainty estimation from GPs useful for certain tasks?<ul id="1368e960-4bb7-44a5-b871-dab41c4e6407" class="bulleted-list"><li>Useful for determining if similar data was previously seen</li></ul><ul id="1ba8b0df-05f8-4db4-b027-52b0a6d4b98b" class="bulleted-list"><li>Maybe not possible to propagate due to skip connections?</li></ul></li></ul><p id="0906c61b-23b2-41b7-88f1-a610f1d166af" class="">
</p><ul id="0fc23667-dbe2-4f81-bd45-e36046e5f2f2" class="bulleted-list"><li>Are there benefits to using GP uncertainty instead of having networks directly predict the uncertainty as an output? (<a href="https://arxiv.org/pdf/1703.04977.pdf">https://arxiv.org/pdf/1703.04977.pdf</a>)<ul id="dcd68000-097d-438a-b385-31f1a70e9397" class="bulleted-list"><li>To model uncertainty, some networks commonly use loss such as
</li></ul></li></ul><figure id="2eec9bc2-1790-482f-884a-9312263e6943" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2017.png"><img style="width:384px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2017.png"/></a></figure><ul id="25c7f4cf-a4f9-4ff9-a926-a236edeb846e" class="bulleted-list"><li>GPs can be viewed as infinitely wide neural network layer with i.i.d. prior on weights</li></ul><ul id="0ad87529-e67f-491f-8c8f-48f0c3261f25" class="bulleted-list"><li>GPs give more flexible models, while Bayesian NNs give more scalable models</li></ul><ul id="5370fbde-f94e-42b9-bbc9-371012b42df9" class="bulleted-list"><li>With enough data, is it sufficient to use neural networks?</li></ul><p id="d3b951ef-eca6-4e2a-9d52-a5b4865b4858" class="">
</p><ul id="fdfa07e9-0bfb-42ed-9eb5-00ec93b3c671" class="bulleted-list"><li>Examples of neural network uncertainty usage in dense SLAM<ul id="7930dbc0-03b9-43a4-86ef-720a14d64a03" class="bulleted-list"><li>D3VO: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf</a></li></ul><ul id="16a4e065-8715-4f91-b4ff-b642afe03af7" class="bulleted-list"><li>CodeSLAM: <a href="https://arxiv.org/pdf/1804.00874.pdf">https://arxiv.org/pdf/1804.00874.pdf</a></li></ul></li></ul></div></article></body></html>